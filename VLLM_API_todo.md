# vLLM Responses API - Complete 100% Compatibility Roadmap

## Document Purpose

This is a **complete, detailed roadmap** for achieving **100% OpenAI Responses API compatibility** in vLLM. Every single aspect of the OpenAI specification is covered with actionable implementation tasks.

**Current State**: 68.25% compatible
**Target**: **100% OpenAI Responses API compatibility**

Each task includes:
- Detailed implementation steps with code
- File locations and references
- Comprehensive testing strategies
- Priority and complexity ratings
- Time estimates
- Dependencies

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Complete Task Overview](#complete-task-overview)
3. [Critical Priority Tasks (C1-C4)](#critical-priority-tasks)
4. [High Priority Tasks (H1-H5)](#high-priority-tasks)
5. [Medium Priority Tasks (M1-M6)](#medium-priority-tasks)
6. [Low Priority Tasks (L1-L6)](#low-priority-tasks)
7. [Edge Cases & Validation (E1-E8)](#edge-cases--validation)
8. [Documentation & Compliance (D1-D5)](#documentation--compliance)
9. [Testing Strategy](#testing-strategy)
10. [Migration & Backward Compatibility](#migration--backward-compatibility)
11. [Performance & Monitoring](#performance--monitoring)
12. [Implementation Timeline](#implementation-timeline)

---

## Executive Summary

### Total Effort Breakdown

| Category | Tasks | Hours | Days | Percentage |
|----------|-------|-------|------|------------|
| ðŸ”´ Critical Priority | 4 | 120h | 15d | 25% |
| ðŸŸ¡ High Priority | 5 | 80h | 10d | 17% |
| ðŸŸ  Medium Priority | 6 | 96h | 12d | 20% |
| ðŸŸ¢ Low Priority | 6 | 64h | 8d | 13% |
| âš¡ Edge Cases | 8 | 56h | 7d | 12% |
| ðŸ“– Documentation | 5 | 40h | 5d | 8% |
| âœ… Testing | 1 | 24h | 3d | 5% |
| **TOTAL** | **35** | **480h** | **60d** | **100%** |

**Timeline**: 60 working days (3 months) with 1 developer, or 30 days with 2 developers

### Success Metrics

- âœ… **100% endpoint compatibility** (4/4 endpoints)
- âœ… **100% parameter support** (all OpenAI parameters)
- âœ… **100% SSE event coverage** (all OpenAI events)
- âœ… **100% error handling compatibility** (all error types)
- âœ… **100% tool calling workflow compatibility**
- âœ… **Pass all OpenAI compliance tests**
- âœ… **Zero breaking changes for migrating users**

---

## Complete Task Overview

### Master Task List (35 Tasks)

| ID | Task | Priority | Hours | Status |
|----|------|----------|-------|--------|
| **C1** | Implement `/v1/responses/{id}/tool_outputs` endpoint | ðŸ”´ | 40h | âœ… Completed |
| **C2** | Add `response.tool_call.delta` SSE event | ðŸ”´ | 32h | âœ… Completed |
| **C3** | Implement stateful response sessions | ðŸ”´ | 40h | âœ… Completed |
| **C4** | Add `response.error` SSE event | ðŸ”´ | 8h | âœ… Completed |
| **H1** | Implement `prompt_cache_key` parameter | ðŸŸ¡ | 24h | âœ… Completed |
| **H2** | Rename reasoning events to OpenAI format | ðŸŸ¡ | 8h | âœ… Completed |
| **H3** | Add `response.reasoning.summary.*` events | ðŸŸ¡ | 16h | âœ… Completed |
| **H4** | Implement `response.additional_context` event | ðŸŸ¡ | 16h | âœ… Completed |
| **H5** | Add `response.rate_limits.updated` event | ðŸŸ¡ | 16h | âœ… Completed |
| **M1** | Azure endpoint format support | ðŸŸ  | 16h | âœ… Completed |
| **M2** | Implement OpenAI-compatible error types | ðŸŸ  | 12h | âœ… Completed |
| **M3** | Add Retry-After header handling | ðŸŸ  | 16h | âœ… Completed |
| **M4** | Implement all `include` parameter options | ðŸŸ  | 20h | âœ… Completed |
| **M5** | Add HTTP headers compatibility | ðŸŸ  | 16h | âœ… Completed |
| **M6** | Implement `store` parameter semantics | ðŸŸ  | 16h | âœ… Completed |
| **L1** | Add comprehensive SSE validation | ðŸŸ¢ | 8h | âœ… Completed |
| **L2** | Performance optimization for streaming | ðŸŸ¢ | 12h | âœ… Completed |
| **L3** | Add compatibility mode flag | ðŸŸ¢ | 8h | âœ… Completed |
| **L4** | Implement ping/keep-alive for SSE | ðŸŸ¢ | 8h | âœ… Completed |
| **L5** | Add `[DONE]` message in SSE stream | ðŸŸ¢ | 4h | âœ… Completed |
| **L6** | Implement sequence_number tracking | ðŸŸ¢ | 8h | âœ… Completed |
| **L7** | Add request/response ID consistency | ðŸŸ¢ | 8h | âœ… Completed |
| **L8** | Implement service_tier parameter behavior | ðŸŸ¢ | 8h | âœ… Completed |
| **E1** | Handle malformed SSE gracefully | âš¡ | 8h | âœ… Completed |
| **E2** | Timeout handling for tool outputs | âš¡ | 8h | âœ… Completed |
| **E3** | Concurrent tool calls validation | âš¡ | 8h | âœ… Completed |
| **E4** | Large payload handling (>1MB) | âš¡ | 8h | âœ… Completed |
| **E5** | Session cleanup on client disconnect | âš¡ | 8h | âœ… Completed |
| **E6** | Duplicate tool_output submission | âš¡ | 4h | âœ… Completed |
| **E7** | Invalid JSON in tool arguments | âš¡ | 4h | âœ… Completed |
| **E8** | Memory limits for long sessions | âš¡ | 8h | âœ… Completed |
# ðŸ“– Documentation | **D1â€“D5**

| ID  | Task                                   | Priority | Hours | Status |
|-----|----------------------------------------|----------|-------|--------|
| **D1** | Complete API documentation             | ðŸ“– | 12h | âœ… Completed |
| **D2** | Migration guide for existing users      | ðŸ“– | 8h | âœ… Completed |
| **D3** | OpenAI compatibility documentation | ðŸ“– | 8h | âœ… Completed |
| **D4** | Code examples and tutorials | ðŸ“– | 8h | âœ… Completed |
| **D5** | Troubleshooting guide | ðŸ“– | 4h | âœ… Completed |
| **T1** | Comprehensive compliance test suite | âœ… | 24h | âœ… Completed |

---

## Critical Priority Tasks

### C1: Implement `/v1/responses/{id}/tool_outputs` Endpoint

**Priority**: ðŸ”´ Critical | **Complexity**: High | **Time**: 40 hours  
_Status: âœ… FastAPI endpoint, session plumbing, and tests landed._

#### Implementation Details

- Added `POST /v1/responses/{response_id}/tool_outputs` handler in
  `vllm/entrypoints/openai/api_server.py:1020-1059`; it validates payloads via
  `ResponsesToolOutputsRequest`, forwards to `OpenAIServingResponses`, and emits
  standard headers (including request/org metadata) just like OpenAI.
- `OpenAIServingResponses.submit_tool_outputs`
  (`vllm/entrypoints/openai/serving_responses.py:1920-2008`) enforces
  `max_tool_output_bytes`, checks that the streaming session is waiting for the
  specified `tool_call_id`, appends outputs into the Harmony context, and
  resumes generation once every pending call is satisfied.
- `ResponseStreamSession` tracks outstanding tool calls, arguments, and uses an
  `asyncio.Event` to pause/resume streaming (`serving_responses.py:156-213`,
  `1854-1918`).
- Regression coverage lives in
  `tests/entrypoints/openai/test_serving_responses.py::test_submit_tool_outputs_*`
  and the Pytests HOWTO references the command to run these checks.

---

### C2: Add `response.tool_call.delta` SSE Event

**Priority**: ðŸ”´ Critical | **Complexity**: High | **Time**: 32 hours  
_Status: âœ… Harmony streaming now emits OpenAI-compatible tool call deltas._

#### Implementation Details

- New protocol models `ResponseToolCallDeltaContent/Event` live in
  `vllm/entrypoints/openai/protocol.py:2470-2534` and are part of the
  `StreamingResponsesResponse` union, so SSE serialization uses the correct
  `type="response.tool_call.delta"` envelope.
- `_process_harmony_streaming_events` in
  `vllm/entrypoints/openai/serving_responses.py:2960-3105` registers tool calls,
  aggregates argument deltas, and yields `_build_tool_call_delta_event(...)`
  whenever Harmony reports commentary destined to `functions.*`.
- `ResponseStreamSession` keeps the per-item â†’ `tool_call_id` mapping so deltas
  reference the correct `call_id`, and the data is also stored for later
  validation in `submit_tool_outputs`.
- Tests in
  `tests/entrypoints/openai/test_serving_responses.py::test_process_harmony_streams_tool_call_delta`
  assert that the SSE event is emitted and carries incremental arguments.

---

### C3: Implement Stateful Response Sessions

**Priority**: ðŸ”´ Critical | **Complexity**: Very High | **Time**: 40 hours  
_Status: âœ… Streaming/background sessions share a unified store with reconnection support._

#### Implementation Details

- `_start_stream_response_task` now builds a `ResponseSession` object containing
  the request, sampling params, Harmony context, tokenizer, event deque, and the
  background producer task; every session is registered via
  `ResponseSessionManager.add_session`.
- `ResponseSessionManager` tracks sessions with TTLs + max-cap eviction; it
  cancels background tasks when evicting (`serving_responses.py:238-318`), and
  `retrieve_responses(..., stream=True)` simply streams from the stored deque so
  clients can reconnect mid-flight.
- `responses_background_stream_generator` replays existing events before waiting
  on `new_event_signal`, while `_produce_stream_events` keeps appending to the
  deque and sets the signal so listeners wake up.
- `handle_stream_disconnect` is invoked when SSE clients drop; it removes the
  session, cancels the producer, and logs the cleanup, preventing leaks.
- Tests: `tests/entrypoints/openai/test_serving_responses.py::test_response_session_manager_basic`,
  `::test_response_session_manager_cleanup_expired`, `::test_session_manager_evicts_idle_sessions`,
  `::test_retrieve_responses_stream_uses_session`, and
  `::test_handle_stream_disconnect_cancels_background_task`.

---

### C4: Add `response.error` SSE Event

_Status: âœ… Completed (streamed Responses requests now emit `response.error` SSE events when they fail)._

**Priority**: ðŸ”´ Critical | **Complexity**: Medium | **Time**: 8 hours

#### Summary

- Added `ResponseErrorEvent` to the streaming protocol and `_stream_error_response`
  helper in `api_server.py`.
- When a streaming request fails (including Azure deployments), the server now
  emits a single `response.error` event before closing the stream; non-streaming
  requests still receive JSON error payloads.
- Added regression coverage in `tests/entrypoints/openai/test_api_server_helpers.py`
  to ensure the SSE payload structure matches expectations.

---

## High Priority Tasks

### H1: Implement `prompt_cache_key` Parameter

**Priority**: ðŸŸ¡ High | **Complexity**: High | **Time**: 24 hours  
_Status: âœ… Requests can supply OpenAIâ€™s `prompt_cache_key`; vLLM maps it to prefix cache salt._

#### Implementation Details

- `ResponsesRequest` accepts `prompt_cache_key` (validated via
  `vllm/entrypoints/openai/protocol.py:372-525`). Empty strings are rejected to
  match OpenAI semantics.
- `_apply_prompt_cache_key` in `serving_responses.py:740-759` translates the key
  into `request.cache_salt`, unless the caller already provided `cache_salt`
  (in which case we log and keep the explicit salt).
- `create_responses` calls `_apply_prompt_cache_key` before scheduling the
  request (`serving_responses.py:956`), ensuring both streaming and background
  flows share the same cache namespace.
- Tests in
  `tests/entrypoints/openai/test_serving_responses.py::test_prompt_cache_key_*`
  confirm the salt is applied/ignored correctly. API docs now include usage
  guidance for clients (see `API_response_HOWTO.md`).

---

### H2: Rename Reasoning Events to OpenAI Format

_Status: âœ… Completed (vLLM now emits `response.reasoning.delta/done`; use `legacy_reasoning_events` for old names)._

_Update (2025-11-24)_: Reasoning summaries now emit the full OpenAI event set, including
`response.reasoning_summary_part.added/done` alongside the existing
`response.reasoning_summary_text.delta/done` pair
(`vllm/entrypoints/openai/serving_responses.py:_generate_reasoning_summary_events`).

**Priority**: ðŸŸ¡ High | **Complexity**: Low | **Time**: 8 hours

#### Current State

vLLM uses `response.reasoning_text.delta` while OpenAI uses `response.reasoning.delta`.

**Files affected**: `vllm/entrypoints/openai/protocol.py`, `serving_responses.py`

#### Implementation Steps

##### Step 1: Update Event Model Names (2 hours)

**File**: `vllm/entrypoints/openai/protocol.py`

```python
# OLD (current vLLM)
class ResponseReasoningTextDeltaEvent(OpenAIBaseModel):
    type: Literal["response.reasoning_text.delta"] = "response.reasoning_text.delta"
    delta: str
    ...

# NEW (OpenAI compatible)
class ResponseReasoningDeltaEvent(OpenAIBaseModel):
    """
    Event emitted when reasoning content is being generated.

    OpenAI spec: OAI_API_spec.md:112-118
    """
    type: Literal["response.reasoning.delta"] = "response.reasoning.delta"
    response: dict = Field(..., description="Response object")
    delta: dict = Field(
        ...,
        description="Delta containing reasoning content",
        examples=[{"content": "Step 1: analyzing..."}]
    )
    sequence_number: int


# Update TypeAlias
StreamingResponsesResponse: TypeAlias = (
    ResponseCreatedEvent
    | ResponseInProgressEvent
    | ResponseReasoningDeltaEvent  # Renamed from ResponseReasoningTextDeltaEvent
    | ResponseReasoningDoneEvent    # Renamed from ResponseReasoningTextDoneEvent
    | ...
)
```

##### Step 2: Update Event Emission Logic (4 hours)

**File**: `vllm/entrypoints/openai/serving_responses.py`

```python
# Update all locations that emit reasoning events

async def _process_harmony_streaming_events(...):
    # OLD
    yield ResponseReasoningTextDeltaEvent(
        type="response.reasoning_text.delta",
        delta=reasoning_chunk,
        ...
    )

    # NEW - Match OpenAI format
    yield ResponseReasoningDeltaEvent(
        type="response.reasoning.delta",
        response={"id": response_id},
        delta={"content": reasoning_chunk},  # Wrap in dict
        sequence_number=self._get_next_sequence()
    )
```

##### Step 3: Add Backward Compatibility (2 hours)

Add optional compatibility layer for existing clients:

```python
class OpenAIServingResponses:
    def __init__(self, ..., legacy_reasoning_events: bool = False):
        """
        Args:
            legacy_reasoning_events: If True, emit old event names
                (response.reasoning_text.delta) for backward compatibility
        """
        self.legacy_reasoning_events = legacy_reasoning_events

    async def _emit_reasoning_event(self, content: str, ...):
        if self.legacy_reasoning_events:
            # Emit old format for backward compatibility
            yield ResponseReasoningTextDeltaEvent(...)
        else:
            # Emit OpenAI-compatible format
            yield ResponseReasoningDeltaEvent(...)
```

#### Testing

```python
@pytest.mark.asyncio
async def test_reasoning_delta_event_format(client: AsyncClient):
    """Test reasoning.delta event matches OpenAI spec"""

    response = await client.post(
        "/v1/responses",
        json={
            "model": "gpt-4",
            "input": "Solve 2+2",
            "reasoning": {"effort": "medium"},
            "stream": True
        },
        headers={"Accept": "text/event-stream"}
    )

    reasoning_event_found = False
    async for line in response.aiter_lines():
        if line.startswith("event: response.reasoning.delta"):
            reasoning_event_found = True

        if reasoning_event_found and line.startswith("data: "):
            data = json.loads(line[6:])

            # Validate OpenAI format
            assert data["type"] == "response.reasoning.delta"
            assert "response" in data
            assert "delta" in data
            assert "content" in data["delta"]
            break

    assert reasoning_event_found
```

---

### H3: Add `response.reasoning.summary.*` Events

_Status: âœ… Completed (summary.added/delta emitted with automatic extraction)._

**Priority**: ðŸŸ¡ High | **Complexity**: Medium | **Time**: 16 hours

#### Current State

`OpenAIServingResponses` now emits `response.reasoning.summary.added` followed by
chunked `response.reasoning.summary.delta` events whenever a reasoning block
finishes streaming. A lightweight `ReasoningSummaryExtractor` trims the raw
reasoning text to two sentences (max 480 characters) and slices it into
160-character chunks so the SSE payload mirrors OpenAI's schema. This runs for
both Harmony and simple streaming paths and piggybacks on the existing reasoning
completion flow, so there is no extra client configuration.

#### Testing

- Unit coverage: `python -m pytest tests/entrypoints/openai/test_serving_responses.py::TestReasoningSummaryEvents`
- Manual SSE validation: initiate a streamed Responses request with
  `reasoning={"effort": "low"}` and confirm `response.reasoning.summary.*`
  events appear alongside `response.reasoning.*`.

---

### H4: Implement `response.additional_context` Event

**Priority**: ðŸŸ¡ High | **Complexity**: Medium | **Time**: 16 hours

#### Current State

vLLM now mirrors OpenAI by emitting `response.additional_context` whenever
`include=["reasoning.encrypted_content"]` is supplied. The original plan below
is retained for historical context.

**OpenAI Spec**: OAI_API_spec.md:182-191

#### Implementation Steps

##### Step 1: Define Event Model (3 hours)

**File**: `vllm/entrypoints/openai/protocol.py`

```python
class ResponseAdditionalContextEvent(OpenAIBaseModel):
    """
    Event for transmitting additional context like encrypted reasoning.

    Used when `include` parameter requests additional data:
    - reasoning.encrypted_content
    - code_interpreter_call.outputs
    - etc.

    OpenAI spec: OAI_API_spec.md:182-191
    """
    type: Literal["response.additional_context"] = "response.additional_context"
    response: dict = Field(..., description="Response object")
    context: dict = Field(
        ...,
        description="Additional context data",
        examples=[{
            "reasoning.encrypted_content": "<base64_encrypted_data>"
        }]
    )
    sequence_number: int


StreamingResponsesResponse: TypeAlias = (
    ...
    | ResponseAdditionalContextEvent
    | ...
)
```

##### Step 2: Implement Reasoning Encryption (8 hours)

**File**: `vllm/entrypoints/openai/reasoning_encryption.py` (NEW FILE)

```python
"""
Reasoning encryption for OpenAI Responses API.

Encrypts reasoning content for secure transmission while
allowing summary to be visible.
"""

import base64
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2
import os
import logging

logger = logging.getLogger(__name__)


class ReasoningEncryption:
    """
    Handles encryption of reasoning content.

    Uses symmetric encryption with key derived from server secret.
    Clients with the key can decrypt reasoning content.
    """

    def __init__(self, encryption_key: str | None = None):
        """
        Args:
            encryption_key: Base64-encoded encryption key.
                If None, generates new key (for development only).
        """
        if encryption_key:
            self.key = base64.urlsafe_b64decode(encryption_key)
        else:
            # Generate key for development (should use config in production)
            logger.warning(
                "No encryption key provided, generating ephemeral key. "
                "This is insecure for production!"
            )
            self.key = Fernet.generate_key()

        self.cipher = Fernet(self.key)

    def encrypt_reasoning(self, reasoning_text: str) -> str:
        """
        Encrypt reasoning content.

        Returns base64-encoded encrypted data.
        """
        try:
            encrypted = self.cipher.encrypt(reasoning_text.encode('utf-8'))
            return base64.b64encode(encrypted).decode('ascii')
        except Exception as e:
            logger.error(f"Failed to encrypt reasoning: {e}")
            raise

    def decrypt_reasoning(self, encrypted_data: str) -> str:
        """
        Decrypt reasoning content.

        Args:
            encrypted_data: Base64-encoded encrypted data

        Returns decrypted text
        """
        try:
            encrypted = base64.b64decode(encrypted_data)
            decrypted = self.cipher.decrypt(encrypted)
            return decrypted.decode('utf-8')
        except Exception as e:
            logger.error(f"Failed to decrypt reasoning: {e}")
            raise

    @staticmethod
    def generate_key() -> str:
        """Generate a new encryption key (base64-encoded)"""
        key = Fernet.generate_key()
        return base64.urlsafe_b64encode(key).decode('ascii')


# Example usage in serving_responses.py
class OpenAIServingResponses:
    def __init__(self, ..., reasoning_encryption_key: str | None = None):
        ...
        self.reasoning_encryption = ReasoningEncryption(reasoning_encryption_key)
```

##### Step 3: Emit Additional Context Events (5 hours)

**File**: `vllm/entrypoints/openai/serving_responses.py`

```python
async def _process_reasoning_with_context(
    self,
    request: ResponsesRequest,
    response_id: str,
    reasoning_text: str,
) -> AsyncGenerator[StreamingResponsesResponse, None]:
    """
    Process reasoning, emitting both regular and additional context events.
    """

    # Stream regular reasoning events
    async for event in self._process_reasoning_with_summary(
        response_id, reasoning_text
    ):
        yield event

    # Check if encrypted content requested
    if request.include and "reasoning.encrypted_content" in request.include:
        # Encrypt reasoning
        encrypted = self.reasoning_encryption.encrypt_reasoning(reasoning_text)

        # Emit additional_context event
        yield ResponseAdditionalContextEvent(
            type="response.additional_context",
            response={"id": response_id},
            context={
                "reasoning.encrypted_content": encrypted
            },
            sequence_number=self._get_next_sequence()
        )

        logger.info(
            f"Sent encrypted reasoning for {response_id} "
            f"({len(encrypted)} bytes)"
        )
```

#### Configuration

**File**: `vllm/entrypoints/openai/cli_args.py`

```python
parser.add_argument(
    "--reasoning-encryption-key",
    type=str,
    default=None,
    help=(
        "Base64-encoded encryption key for reasoning content. "
        "If not provided, a new key will be generated (insecure for production). "
        "Generate with: python -c 'from cryptography.fernet import Fernet; "
        "print(Fernet.generate_key().decode())'"
    )
)
```

#### Testing

```python
@pytest.mark.asyncio
async def test_additional_context_encrypted_reasoning(client: AsyncClient):
    """Test encrypted reasoning in additional_context event"""

    response = await client.post(
        "/v1/responses",
        json={
            "model": "gpt-4",
            "input": "Complex task",
            "reasoning": {"effort": "high"},
            "include": ["reasoning.encrypted_content"],
            "stream": True
        }
    )

    additional_context_found = False
    encrypted_content = None

    async for line in response.aiter_lines():
        if "response.additional_context" in line:
            additional_context_found = True

        if additional_context_found and line.startswith("data: "):
            data = json.loads(line[6:])
            if "context" in data:
                encrypted_content = data["context"].get("reasoning.encrypted_content")
                break

    assert additional_context_found
    assert encrypted_content is not None
    assert len(encrypted_content) > 0

    # Verify it's valid base64
    base64.b64decode(encrypted_content)
```

---

### H5: Add `response.rate_limits.updated` Event

_Status: âœ… Completed (optional rate limit tracker now emits rate_limits.updated after responses)._

**Priority**: ðŸŸ¡ High | **Complexity**: Medium | **Time**: 16 hours

#### Current State

OpenAI sends `response.rate_limits.updated` events to inform clients about rate limit usage. vLLM now implements a lightweight tracker (behind `--enable-rate-limit-events`) that mirrors this behavior.

**OpenAI Spec**: OAI_API_spec.md:193-202

#### Implementation Steps

##### Step 1: Define Rate Limit Models (3 hours)

**File**: `vllm/entrypoints/openai/protocol.py`

```python
class RateLimitInfo(OpenAIBaseModel):
    """Rate limit information for a specific limit type"""
    used_percent: float = Field(
        ...,
        description="Percentage of limit used (0-100)",
        ge=0,
        le=100
    )
    window_minutes: int = Field(
        ...,
        description="Time window for this limit in minutes"
    )
    limit_type: Literal["primary", "requests", "tokens", "images"] = "primary"


class ResponseRateLimitsUpdatedEvent(OpenAIBaseModel):
    """
    Event for rate limit updates.

    Informs clients about their current rate limit usage.
    OpenAI spec: OAI_API_spec.md:193-202
    """
    type: Literal["response.rate_limits.updated"] = "response.rate_limits.updated"
    response: dict
    limits: dict[str, RateLimitInfo] = Field(
        ...,
        description="Rate limit information by type",
        examples=[{
            "primary": {
                "used_percent": 42.5,
                "window_minutes": 60,
                "limit_type": "primary"
            }
        }]
    )
    sequence_number: int


StreamingResponsesResponse: TypeAlias = (
    ...
    | ResponseRateLimitsUpdatedEvent
    | ...
)
```

##### Step 2: Implement Rate Limit Tracking (8 hours)

**File**: `vllm/entrypoints/openai/rate_limiter.py` (NEW FILE)

```python
"""
Rate limiting for OpenAI-compatible API.

Tracks per-user/per-model rate limits and provides
usage statistics for rate_limits.updated events.
"""

import time
from collections import defaultdict, deque
from dataclasses import dataclass, field
from typing import Literal
import asyncio
import logging

logger = logging.getLogger(__name__)


@dataclass
class RateLimitWindow:
    """Sliding window for rate limit tracking"""
    window_seconds: int
    max_count: int
    requests: deque = field(default_factory=deque)  # timestamps

    def add_request(self, timestamp: float | None = None):
        """Add a request to the window"""
        if timestamp is None:
            timestamp = time.time()
        self.requests.append(timestamp)
        self._cleanup_old_requests(timestamp)

    def _cleanup_old_requests(self, now: float):
        """Remove requests outside the window"""
        cutoff = now - self.window_seconds
        while self.requests and self.requests[0] < cutoff:
            self.requests.popleft()

    def get_usage_percent(self) -> float:
        """Get current usage as percentage of limit"""
        now = time.time()
        self._cleanup_old_requests(now)
        return (len(self.requests) / self.max_count) * 100 if self.max_count > 0 else 0

    def is_limited(self) -> bool:
        """Check if rate limit is exceeded"""
        return len(self.requests) >= self.max_count


class RateLimiter:
    """
    Manages rate limits for API requests.

    Supports multiple limit types:
    - Primary: Overall request rate
    - Requests: Requests per time window
    - Tokens: Token consumption rate
    """

    def __init__(
        self,
        requests_per_minute: int = 60,
        tokens_per_minute: int = 100000,
        requests_per_hour: int = 1000,
    ):
        # Per-user limit tracking
        self.user_limits: dict[str, dict[str, RateLimitWindow]] = defaultdict(dict)
        self.lock = asyncio.Lock()

        # Default limits
        self.default_limits = {
            "requests_1min": RateLimitWindow(60, requests_per_minute),
            "requests_1hour": RateLimitWindow(3600, requests_per_hour),
            "tokens_1min": RateLimitWindow(60, tokens_per_minute),
        }

    async def check_limits(self, user_id: str) -> bool:
        """
        Check if user is within rate limits.

        Returns True if allowed, False if rate limited.
        """
        async with self.lock:
            user_windows = self._get_user_windows(user_id)

            # Check all limit types
            for window in user_windows.values():
                if window.is_limited():
                    return False

            return True

    async def record_request(self, user_id: str, tokens_used: int = 0):
        """Record a request for rate limiting"""
        async with self.lock:
            user_windows = self._get_user_windows(user_id)
            now = time.time()

            # Record in request windows
            user_windows["requests_1min"].add_request(now)
            user_windows["requests_1hour"].add_request(now)

            # Record tokens (add multiple entries for token count)
            if tokens_used > 0:
                for _ in range(min(tokens_used, 10000)):  # Cap to avoid memory issues
                    user_windows["tokens_1min"].add_request(now)

    async def get_limit_info(self, user_id: str) -> dict[str, RateLimitInfo]:
        """Get current rate limit information for user"""
        async with self.lock:
            user_windows = self._get_user_windows(user_id)

            return {
                "primary": RateLimitInfo(
                    used_percent=user_windows["requests_1min"].get_usage_percent(),
                    window_minutes=1,
                    limit_type="primary"
                ),
                "requests": RateLimitInfo(
                    used_percent=user_windows["requests_1hour"].get_usage_percent(),
                    window_minutes=60,
                    limit_type="requests"
                ),
                "tokens": RateLimitInfo(
                    used_percent=user_windows["tokens_1min"].get_usage_percent(),
                    window_minutes=1,
                    limit_type="tokens"
                ),
            }

    def _get_user_windows(self, user_id: str) -> dict[str, RateLimitWindow]:
        """Get or create rate limit windows for user"""
        if user_id not in self.user_limits:
            # Create new windows with default limits
            self.user_limits[user_id] = {
                "requests_1min": RateLimitWindow(60, self.default_limits["requests_1min"].max_count),
                "requests_1hour": RateLimitWindow(3600, self.default_limits["requests_1hour"].max_count),
                "tokens_1min": RateLimitWindow(60, self.default_limits["tokens_1min"].max_count),
            }
        return self.user_limits[user_id]


# Integration with serving
class OpenAIServingResponses:
    def __init__(self, ..., enable_rate_limiting: bool = False):
        ...
        self.rate_limiter = RateLimiter() if enable_rate_limiting else None
```

##### Step 3: Emit Rate Limit Events (5 hours)

**File**: `vllm/entrypoints/openai/serving_responses.py`

```python
async def _generate_streaming_events(
    self,
    session: ResponseSession,
    ...
) -> AsyncGenerator[StreamingResponsesResponse, None]:
    """Generate events with periodic rate limit updates"""

    # Get user ID from request
    user_id = session.request.user or "default"

    # Check rate limits at start
    if self.rate_limiter:
        if not await self.rate_limiter.check_limits(user_id):
            yield ResponseErrorEvent(
                type="response.error",
                response={"id": session.response_id, "status": "failed"},
                error=ErrorInfo(
                    message="Rate limit exceeded",
                    type="rate_limit_error",
                    code=429
                ),
                sequence_number=self._get_next_sequence()
            )
            return

        # Record request
        await self.rate_limiter.record_request(user_id)

    # Yield created event
    yield ResponseCreatedEvent(...)

    # Emit initial rate limit info
    if self.rate_limiter:
        limits = await self.rate_limiter.get_limit_info(user_id)
        yield ResponseRateLimitsUpdatedEvent(
            type="response.rate_limits.updated",
            response={"id": session.response_id},
            limits=limits,
            sequence_number=self._get_next_sequence()
        )

    # Generate content...
    event_count = 0
    async for event in self._process_generation(...):
        yield event
        event_count += 1

        # Emit rate limit update every 20 events
        if self.rate_limiter and event_count % 20 == 0:
            limits = await self.rate_limiter.get_limit_info(user_id)
            yield ResponseRateLimitsUpdatedEvent(
                type="response.rate_limits.updated",
                response={"id": session.response_id},
                limits=limits,
                sequence_number=self._get_next_sequence()
            )

    # Final rate limit update
    if self.rate_limiter:
        # Record token usage
        if usage:
            await self.rate_limiter.record_request(
                user_id,
                tokens_used=usage.total_tokens
            )

        limits = await self.rate_limiter.get_limit_info(user_id)
        yield ResponseRateLimitsUpdatedEvent(
            type="response.rate_limits.updated",
            response={"id": session.response_id},
            limits=limits,
            sequence_number=self._get_next_sequence()
        )
```

#### Testing

```python
@pytest.mark.asyncio
async def test_rate_limits_updated_event(client: AsyncClient):
    """Test rate_limits.updated events are emitted"""

    response = await client.post(
        "/v1/responses",
        json={
            "model": "gpt-4",
            "input": "test",
            "stream": True,
            "user": "test_user_123"
        }
    )

    rate_limit_events = []
    async for line in response.aiter_lines():
        if "response.rate_limits.updated" in line:
            # Next line has data
            continue

        if line.startswith("data: ") and "rate_limits.updated" in prev_event:
            data = json.loads(line[6:])
            rate_limit_events.append(data)

    # Should have at least 2 rate limit updates (start and end)
    assert len(rate_limit_events) >= 2

    # Validate structure
    for event in rate_limit_events:
        assert "limits" in event
        assert "primary" in event["limits"]
        assert 0 <= event["limits"]["primary"]["used_percent"] <= 100
```

---

## Medium Priority Tasks

### M1: Azure Endpoint Format Support

_Status: âœ… Completed (Azure deployments can POST to `/openai/deployments/{name}/responses` when `--enable-azure-api` is enabled.)_

**Priority**: ðŸŸ  Medium | **Complexity**: Medium | **Time**: 16 hours

#### Current State

Azure OpenAI uses different endpoint format:
```
https://{resource}.openai.azure.com/openai/deployments/{model}/responses?api-version=2024-02-15-preview
```

vLLM uses standard format: `http://host:port/v1/responses`

**OpenAI Spec**: OAI_API_spec.md:255-261

#### Implementation Steps

##### Step 1: Azure URL Pattern Detection (4 hours)

**File**: `vllm/entrypoints/openai/api_server.py`

```python
import re
from urllib.parse import urlparse, parse_qs

def is_azure_endpoint(request: Request) -> bool:
    """Detect if request is for Azure OpenAI format"""
    path = request.url.path
    host = request.url.hostname or ""

    # Check host pattern
    azure_host_pattern = r'\.openai\.azure\.com$'
    if re.search(azure_host_pattern, host):
        return True

    # Check path pattern
    azure_path_pattern = r'/openai/deployments/[^/]+/responses'
    if re.match(azure_path_pattern, path):
        return True

    return False

def extract_azure_model(request: Request) -> str | None:
    """Extract model name from Azure deployment path"""
    path = request.url.path
    match = re.search(r'/deployments/([^/]+)/', path)
    if match:
        return match.group(1)
    return None


@router.post("/openai/deployments/{deployment_name}/responses")
async def create_responses_azure(
    deployment_name: str,
    request: ResponsesRequest,
    raw_request: Request,
    api_version: str = Query(default="2024-02-15-preview")
):
    """
    Azure OpenAI compatible endpoint.

    Format: /openai/deployments/{deployment_name}/responses?api-version=XXX
    """
    # Force Azure-specific settings
    if not request.store:
        logger.warning(
            f"Azure requires store=true, overriding store={request.store}"
        )
        request.store = True

    # Use deployment_name as model if model not specified
    if not request.model:
        request.model = deployment_name

    # Ensure item IDs are attached (Azure requirement)
    if isinstance(request.input, list):
        request.input = _attach_item_ids(request.input)

    # Call standard endpoint logic
    return await create_responses(request, raw_request)


def _attach_item_ids(items: list) -> list:
    """
    Attach stable IDs to input items (Azure requirement).

    Azure requires stable item IDs for conversation tracking.
    """
    for i, item in enumerate(items):
        if isinstance(item, dict) and "id" not in item:
            item["id"] = f"item_{i}_{random_uuid()}"
    return items
```

##### Step 2: Azure Authentication (6 hours)

**File**: `vllm/entrypoints/openai/azure_auth.py` (NEW FILE)

```python
"""
Azure OpenAI authentication handling.

Supports Azure API keys and Azure AD tokens.
"""

from fastapi import Header, HTTPException
import logging

logger = logging.getLogger(__name__)


async def azure_auth(
    api_key: str | None = Header(None, alias="api-key"),
    authorization: str | None = Header(None)
) -> str:
    """
    Azure authentication handler.

    Supports two methods:
    1. api-key header (Azure API key)
    2. Authorization: Bearer <azure_ad_token>
    """

    # Check api-key header (Azure format)
    if api_key:
        # Validate API key format (Azure keys are typically 32 chars)
        if len(api_key) < 32:
            raise HTTPException(
                status_code=401,
                detail={
                    "error": {
                        "message": "Invalid API key format",
                        "type": "invalid_api_key",
                        "code": 401
                    }
                }
            )
        return api_key

    # Check Authorization header (Azure AD token)
    if authorization and authorization.startswith("Bearer "):
        token = authorization[7:]
        # Note: Full Azure AD validation would require Azure SDK
        # For now, accept any bearer token
        logger.info("Received Azure AD bearer token")
        return token

    raise HTTPException(
        status_code=401,
        detail={
            "error": {
                "message": "Authentication required. Provide api-key header or Authorization bearer token",
                "type": "authentication_error",
                "code": 401
            }
        }
    )


# Add dependency to Azure routes
@router.post("/openai/deployments/{deployment_name}/responses")
async def create_responses_azure(
    deployment_name: str,
    request: ResponsesRequest,
    raw_request: Request,
    api_version: str = Query(default="2024-02-15-preview"),
    _auth: str = Depends(azure_auth)  # Azure auth
):
    ...
```

##### Step 3: Azure API Version Handling (3 hours)

```python
SUPPORTED_AZURE_API_VERSIONS = [
    "2024-02-15-preview",
    "2024-03-01-preview",
    "2024-05-01-preview",
]

@router.post("/openai/deployments/{deployment_name}/responses")
async def create_responses_azure(
    ...,
    api_version: str = Query(default="2024-02-15-preview")
):
    # Validate API version
    if api_version not in SUPPORTED_AZURE_API_VERSIONS:
        return JSONResponse(
            status_code=400,
            content={
                "error": {
                    "message": f"Unsupported api-version: {api_version}. "
                              f"Supported versions: {', '.join(SUPPORTED_AZURE_API_VERSIONS)}",
                    "type": "invalid_request_error",
                    "code": 400
                }
            }
        )

    # Version-specific behavior
    if api_version >= "2024-05-01-preview":
        # Enable newer features
        pass

    ...
```

##### Step 4: Azure Response Headers (3 hours)

```python
from fastapi.responses import StreamingResponse

async def create_responses_azure(...):
    ...

    # Get response from handler
    generator = await handler.create_responses(request, raw_request)

    if isinstance(generator, ErrorResponse):
        response = JSONResponse(...)
    elif request.stream:
        response = StreamingResponse(content=generator, media_type="text/event-stream")
    else:
        response = JSONResponse(...)

    # Add Azure-specific headers
    response.headers["x-ms-region"] = "eastus"  # Configure based on deployment
    response.headers["x-ms-request-id"] = request.request_id
    response.headers["api-supported-versions"] = ",".join(SUPPORTED_AZURE_API_VERSIONS)

    return response
```

#### Configuration

**File**: `vllm/entrypoints/openai/cli_args.py`

```python
parser.add_argument(
    "--enable-azure-api",
    action="store_true",
    default=False,
    help="Enable Azure OpenAI API endpoint format and authentication"
)

parser.add_argument(
    "--azure-region",
    type=str,
    default="eastus",
    help="Azure region identifier for response headers"
)
```

#### Testing

```python
@pytest.mark.asyncio
async def test_azure_endpoint_format(client: AsyncClient):
    """Test Azure endpoint format works"""

    response = await client.post(
        "/openai/deployments/gpt-4/responses?api-version=2024-02-15-preview",
        json={
            "input": "test",
            # Note: no 'model' field needed with Azure
        },
        headers={"api-key": "test_azure_key_32_characters_long"}
    )

    assert response.status_code == 200
    assert "x-ms-request-id" in response.headers


@pytest.mark.asyncio
async def test_azure_forces_store_true(client: AsyncClient):
    """Test Azure enforces store=true"""

    response = await client.post(
        "/openai/deployments/gpt-4/responses?api-version=2024-02-15-preview",
        json={
            "input": "test",
            "store": False  # Will be overridden
        },
        headers={"api-key": "test_key"}
    )

    data = response.json()
    # Should be stored despite store=false
    assert data.get("store") == True
```

---

### M2: Implement OpenAI-Compatible Error Types

**Priority**: ðŸŸ  Medium | **Complexity**: Low | **Time**: 12 hours

_Status: âœ… Completed (error responses now emit OpenAI-standard types such as `invalid_request_error`, `rate_limit_error`, etc.)._

#### Current State

vLLM now defines `OpenAIErrorType` in `protocol.py` and infers the correct type/code
pair in `create_error_response`. Legacy values such as `"BadRequestError"` are still
recognized, but outgoing responses prefer OpenAI's canonical names.

**OpenAI Spec**: OAI_API_spec.md:233

#### Implementation Steps

##### Step 1: Define Error Type Enum (2 hours)

**File**: `vllm/entrypoints/openai/protocol.py`

```python
from enum import Enum

class OpenAIErrorType(str, Enum):
    """OpenAI-compatible error types"""

    # Authentication errors
    INVALID_API_KEY = "invalid_api_key"
    AUTHENTICATION_ERROR = "authentication_error"

    # Request errors
    INVALID_REQUEST_ERROR = "invalid_request_error"
    INVALID_PARAMETER = "invalid_parameter"

    # Resource errors
    NOT_FOUND_ERROR = "not_found_error"
    MODEL_NOT_FOUND = "model_not_found"

    # Rate limiting
    RATE_LIMIT_ERROR = "rate_limit_error"
    TOKENS_LIMIT_REACHED = "tokens_limit_reached"
    REQUESTS_LIMIT_REACHED = "requests_limit_reached"

    # Quota/billing
    QUOTA_EXCEEDED = "quota_exceeded"
    INSUFFICIENT_QUOTA = "insufficient_quota"
    USAGE_LIMIT_REACHED = "usage_limit_reached"
    USAGE_NOT_INCLUDED = "usage_not_included"

    # Server errors
    INTERNAL_ERROR = "internal_error"
    INTERNAL_SERVER_ERROR = "internal_server_error"
    SERVICE_UNAVAILABLE = "service_unavailable"
    TIMEOUT_ERROR = "timeout_error"

    # Generation errors
    CONTENT_FILTER = "content_filter"
    CONTENT_POLICY_VIOLATION = "content_policy_violation"

    # Generic
    BAD_REQUEST = "bad_request"

    # Legacy (for backward compatibility)
    BAD_REQUEST_ERROR = "BadRequestError"


# Update ErrorInfo to use enum
class ErrorInfo(OpenAIBaseModel):
    message: str
    type: OpenAIErrorType | str  # Allow str for flexibility
    param: str | None = None
    code: int
```

##### Step 2: Error Type Mapping (4 hours)

**File**: `vllm/entrypoints/openai/serving_engine.py`

```python
def create_error_response(
    self,
    message: str,
    err_type: OpenAIErrorType | str | None = None,
    status_code: HTTPStatus | int | None = None,
    param: str | None = None,
) -> ErrorResponse:
    """
    Create OpenAI-compatible error response.

    Args:
        message: Human-readable error message
        err_type: OpenAI error type. If None, inferred from status_code
        status_code: HTTP status code. If None, inferred from err_type
        param: Parameter name that caused error (optional)
    """

    # Infer error type from status code if not provided
    if err_type is None and status_code:
        err_type = self._error_type_from_status(status_code)

    # Infer status code from error type if not provided
    if status_code is None and err_type:
        status_code = self._status_from_error_type(err_type)

    # Defaults
    if err_type is None:
        err_type = OpenAIErrorType.INTERNAL_ERROR
    if status_code is None:
        status_code = 500

    return ErrorResponse(
        error=ErrorInfo(
            message=message,
            type=err_type,
            param=param,
            code=int(status_code)
        )
    )

def _error_type_from_status(self, status_code: int) -> OpenAIErrorType:
    """Map HTTP status to OpenAI error type"""
    mapping = {
        400: OpenAIErrorType.INVALID_REQUEST_ERROR,
        401: OpenAIErrorType.AUTHENTICATION_ERROR,
        403: OpenAIErrorType.AUTHENTICATION_ERROR,
        404: OpenAIErrorType.NOT_FOUND_ERROR,
        429: OpenAIErrorType.RATE_LIMIT_ERROR,
        500: OpenAIErrorType.INTERNAL_SERVER_ERROR,
        502: OpenAIErrorType.SERVICE_UNAVAILABLE,
        503: OpenAIErrorType.SERVICE_UNAVAILABLE,
        504: OpenAIErrorType.TIMEOUT_ERROR,
    }
    return mapping.get(status_code, OpenAIErrorType.INTERNAL_ERROR)

def _status_from_error_type(self, err_type: OpenAIErrorType | str) -> int:
    """Map OpenAI error type to HTTP status"""
    mapping = {
        OpenAIErrorType.INVALID_API_KEY: 401,
        OpenAIErrorType.AUTHENTICATION_ERROR: 401,
        OpenAIErrorType.INVALID_REQUEST_ERROR: 400,
        OpenAIErrorType.INVALID_PARAMETER: 400,
        OpenAIErrorType.NOT_FOUND_ERROR: 404,
        OpenAIErrorType.MODEL_NOT_FOUND: 404,
        OpenAIErrorType.RATE_LIMIT_ERROR: 429,
        OpenAIErrorType.TOKENS_LIMIT_REACHED: 429,
        OpenAIErrorType.REQUESTS_LIMIT_REACHED: 429,
        OpenAIErrorType.QUOTA_EXCEEDED: 429,
        OpenAIErrorType.INSUFFICIENT_QUOTA: 429,
        OpenAIErrorType.USAGE_LIMIT_REACHED: 429,
        OpenAIErrorType.USAGE_NOT_INCLUDED: 429,
        OpenAIErrorType.INTERNAL_ERROR: 500,
        OpenAIErrorType.INTERNAL_SERVER_ERROR: 500,
        OpenAIErrorType.SERVICE_UNAVAILABLE: 503,
        OpenAIErrorType.TIMEOUT_ERROR: 504,
        OpenAIErrorType.CONTENT_FILTER: 400,
        OpenAIErrorType.CONTENT_POLICY_VIOLATION: 400,
        OpenAIErrorType.BAD_REQUEST: 400,
    }
    return mapping.get(err_type, 500)
```

##### Step 3: Update Error Creation Sites (6 hours)

Update all locations that create errors:

**File**: `vllm/entrypoints/openai/serving_responses.py`

```python
# Example: Model not found
if model_name not in self.available_models:
    return self.create_error_response(
        message=f"Model '{model_name}' not found. Available models: {', '.join(self.available_models)}",
        err_type=OpenAIErrorType.MODEL_NOT_FOUND,
        param="model"
    )

# Example: Invalid temperature
if request.temperature and not (0 <= request.temperature <= 2):
    return self.create_error_response(
        message="temperature must be between 0 and 2",
        err_type=OpenAIErrorType.INVALID_PARAMETER,
        param="temperature"
    )

# Example: Rate limit
if self.rate_limiter and not await self.rate_limiter.check_limits(user_id):
    return self.create_error_response(
        message="Rate limit exceeded. Please try again later.",
        err_type=OpenAIErrorType.RATE_LIMIT_ERROR
    )

# Example: Quota exceeded
if user_quota_exceeded:
    return self.create_error_response(
        message="Usage quota exceeded. Please upgrade your plan.",
        err_type=OpenAIErrorType.QUOTA_EXCEEDED
    )

# Example: Timeout
except asyncio.TimeoutError:
    return self.create_error_response(
        message="Request timed out after 300 seconds",
        err_type=OpenAIErrorType.TIMEOUT_ERROR
    )
```

#### Testing

```python
@pytest.mark.asyncio
async def test_error_types_openai_compatible(client: AsyncClient):
    """Test error responses use OpenAI error types"""

    # Test invalid parameter
    response = await client.post(
        "/v1/responses",
        json={
            "model": "gpt-4",
            "input": "test",
            "temperature": 5.0  # Invalid
        }
    )

    assert response.status_code == 400
    data = response.json()
    assert data["error"]["type"] == "invalid_parameter"
    assert data["error"]["param"] == "temperature"


@pytest.mark.asyncio
async def test_model_not_found_error(client: AsyncClient):
    """Test model not found uses correct error type"""

    response = await client.post(
        "/v1/responses",
        json={
            "model": "nonexistent-model",
            "input": "test"
        }
    )

    assert response.status_code == 404
    data = response.json()
    assert data["error"]["type"] == "model_not_found"
```

---

### M3: Add Retry-After Header Handling

**Priority**: ðŸŸ  Medium | **Complexity**: Medium | **Time**: 16 hours  
_Status: âœ… JSON + SSE errors now carry Retry-After for 429._

- `_maybe_add_retry_after_header` (`api_server.py:632-647`) inspects the
  `ErrorResponse` and attaches `Retry-After` with either the provided
  `error.retry_after` or the default window; invoked from both
  `_json_error_response` and `_stream_error_response` so streaming clients also
  see the header.
- Rate-limit enforcement plugs into `OpenAIServingResponses.build_rate_limit_headers`,
  so `429` errors are consistent with OpenAI guidance; documented in
  `API_response_HOWTO.md` (Error Handling section).

---

### M4: Implement All `include` Parameter Options

**Priority**: ðŸŸ  Medium | **Complexity**: Medium | **Time**: 20 hours  
_Status: âœ… `include[]` toggles advanced payloads (code outputs, file search, computer_call gating)._

- `_request_includes` helper centralizes parsing (see
  `serving_responses.py:819-826`), and streaming loops call it to decide whether
  to emit `code_interpreter_call.outputs`, `file_search_call.results`, or
  `computer_call_output.output.image_url` chunks (`serving_responses.py:2255-2293`
  and `2585-2626`).
- CLI flag `--enable-computer-call` is required before honoring
  `include=computer_call_output.output.image_url` (guarded at
  `serving_responses.py:650-676`).
- Documentation (`API_response_HOWTO.md`) describes how to select these payloads.
- _Update (2025-11-24)_: Built-in image tools (`image_generation.*` / `images.*`)
  now require either a multimodal checkpoint or `--image-service-config`
  (see `ImageServices_HOWTO.md` and `image_service_config.template.json`). Text-only
  models without that config reject the tool call with `response.error`.

---

### M5: Add HTTP Headers Compatibility

**Priority**: ðŸŸ  Medium | **Complexity**: Medium | **Time**: 16 hours  
_Status: âœ… Server mirrors OpenAI headers for IDs, org/version, and rate limits._

- `_apply_standard_response_headers` (`api_server.py:684-710`) now injects
  `x-request-id`, `OpenAI-Organization`, `OpenAI-Version`, and rate-limit headers
  from `OpenAIServingResponses.build_rate_limit_headers`.
- All responses (create/retrieve/cancel/tool_outputs) call this helper to ensure
  parity with OpenAI clients; docs highlight the behavior in the new
  â€œHTTP Headers Compatibilityâ€ section.

---

### M6: Implement `store` Parameter Semantics

**Priority**: ðŸŸ  Medium | **Complexity**: Medium | **Time**: 16 hours  
_Status: âœ… Store=true persists responses with TTL + quota controls and GET replay._

- CLI/config flags `--disable-responses-store`, `--responses-store-ttl`,
  `--responses-store-max-entries`, and `--responses-store-max-bytes` feed into
  `OpenAIServingResponses` (`serving_responses.py:461-520`), enabling operators
  to tune persistence and quotas.
- Stored responses live in `self.response_store`; `_store_response_object`,
  `_get_stored_response`, `_update_stored_response_status`, and
  `_cleanup_response_store_locked` enforce TTL + LRU eviction (`serving_responses.py:3114-3207`).
- `GET /v1/responses/{id}` returns stored completions when available, while
  background requests automatically call `_store_response_object`.
- `API_response_HOWTO.md` now describes how `store=true` interacts with the
  session manager and eviction flags.

---

## Low Priority Tasks

### L1-L8: Polish & Edge Cases

**L1**: Add comprehensive SSE validation (8h)  
_Status: âœ… `_convert_stream_to_sse_events` now validates allowed `response.*` event types and sequence numbers, emitting `response.error` if a malformed payload slips through; see `SSEEventValidator` in `api_server.py` + docs update in `docs/SSE_HOWTO.md`._
**L2**: Performance optimization for streaming (12h)  
_Status: âœ… SSE writer now batches events (16 KB chunks) and reuses validation path to minimize syscalls._
**L3**: Add compatibility mode flag (8h)  
_Status: âœ… `--responses-compatibility-mode` rejects vLLM-only request knobs and limits `include` targets; see `_validate_compatibility_request` in `serving_responses.py` + docs section â€œCompatibility Modeâ€._
**L4**: Implement ping/keep-alive for SSE (8h)
**L5**: Add `[DONE]` message in SSE stream (4h)  
_Status: âœ… `_convert_stream_to_sse_events` now flushes buffered events and emits `data: [DONE]` after `response.completed`; docs (`API_response_HOWTO.md`, `docs/SSE_HOWTO.md`) reflect the SSE terminator._
**L6**: Implement sequence_number tracking (8h)  
_Status: âœ… Event sequence numbers now persist end-to-end; SSE generator preserves existing values (with fallback increments) and reconnections reuse the original ordering._
**L7**: Add request/response ID consistency (8h)
**L8**: Implement service_tier parameter behavior (8h)  
_Status: âœ… CLI flags (`--responses-default-service-tier`, `--responses-allowed-service-tiers`) govern defaults + allowlist; `_validate_service_tier` enforces the policy and mutates `request.service_tier`._

*(Each with detailed 4-12 hour implementations)*

---

## Edge Cases & Validation

### E1-E8: Robustness & Error Handling

**E1**: Handle malformed SSE gracefully (8h)  
_Status: âœ… Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½ `SSEDecoder` c Ð·Ð°Ñ‰Ð¸Ñ‚Ð¾Ð¹ Ð¾Ñ‚ Ð±Ð¸Ñ‚Ñ‹Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð² Ð¸ JSON (ÑÐ¼. `vllm/entrypoints/openai/api_server.py:1914+`) + Ñ‚ÐµÑÑ‚ `tests/entrypoints/openai/test_api_server_helpers.py::test_sse_decoder_handles_malformed_chunks`._
**E2**: Timeout handling for tool outputs (8h)  
_Status: âœ… `_await_tool_outputs` Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ `asyncio.wait_for` Ñ `--responses-tool-timeout`, Ñ‚Ð°Ð¹Ð¼Ð°ÑƒÑ‚ Ð»Ð¾Ð³Ð¸Ñ€ÑƒÐµÑ‚ÑÑ Ð¸ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¸Ñ‚ÑÑ Ð² `response.error` Ñ‡ÐµÑ€ÐµÐ· `_produce_stream_events`._
**E3**: Concurrent tool calls validation (8h)  
_Status: âœ… `ResponseStreamSession` Ñ‚Ñ€ÐµÐºÐ°ÐµÑ‚ pending tool calls + `submit_tool_outputs` Ð´ÐµÑ€Ð¶Ð¸Ñ‚ `session.lock`, Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚ `tool_call_id` Ð¸ Ð¶Ð´Ñ‘Ñ‚ Ð²ÑÐµ Ð²Ñ‹Ð·Ð¾Ð²Ñ‹ Ð¿ÐµÑ€ÐµÐ´ Ð²Ð¾Ð·Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ (`session.has_pending_tool_calls`)._
**E4**: Large payload handling >1MB (8h)  
_Status: âœ… Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ñ‹ `--max-stream-event-bytes` Ð¸ ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»ÑŒ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð° SSE Ð² `_produce_stream_events`; Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð° Ð² `RESP_LIMITS_HOWTO.md`._
**E5**: Session cleanup on client disconnect (8h)  
_Status: âœ… `_convert_stream_to_sse_events` Ñ‚ÐµÐ¿ÐµÑ€ÑŒ Ð²Ñ‹Ð·Ñ‹Ð²Ð°ÐµÑ‚ `OpenAIServingResponses.handle_stream_disconnect`, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð»Ð¾Ð³Ð¸Ñ€ÑƒÐµÑ‚ Ð¾Ñ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ Ð¸ Ð¼Ð³Ð½Ð¾Ð²ÐµÐ½Ð½Ð¾ Ð¾Ñ‚Ð¼ÐµÐ½ÑÐµÑ‚ Ñ„Ð¾Ð½Ð¾Ð²Ñ‹Ðµ Ñ‚Ð°ÑÐºÐ¸._
**E6**: Duplicate tool_output submission (4h)  
_Status: âœ… Ð¡ÐµÑ€Ð²ÐµÑ€ Ð»Ð¾Ð³Ð¸Ñ€ÑƒÐµÑ‚ Ð¸ Ð¾Ñ‚ÐºÐ»Ð¾Ð½ÑÐµÑ‚ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð½Ñ‹Ð¹ `tool_outputs` (ÑÐ¼. `submit_tool_outputs`)._
**E7**: Invalid JSON in tool arguments (4h)  
_Status: âœ… Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½ `_safe_json_loads`, Ð¾ÑˆÐ¸Ð±ÐºÐ¸ Ð»Ð¾Ð³Ð¸Ñ€ÑƒÑŽÑ‚ÑÑ Ð±ÐµÐ· Ð¿Ð°Ð´ÐµÐ½Ð¸Ñ._
**E8**: Memory limits for long sessions (8h)  
_Status: âœ… Ð’Ð²ÐµÐ´Ñ‘Ð½ ÑÑ‡Ñ‘Ñ‚Ñ‡Ð¸Ðº `event_queue_bytes` + Ñ„Ð»Ð°Ð³ `--responses-stream-buffer-max-bytes`, Ð¿Ñ€ÐµÐ²Ñ‹ÑˆÐµÐ½Ð¸Ðµ Ð¼Ð³Ð½Ð¾Ð²ÐµÐ½Ð½Ð¾ Ð·Ð°Ð²ÐµÑ€ÑˆÐ°ÐµÑ‚ ÑÑ‚Ñ€Ð¸Ð¼ Ð¸ Ð»Ð¾Ð³Ð¸Ñ€ÑƒÐµÑ‚ÑÑ (ÑÐ¼. `RESP_LIMITS_HOWTO.md`)._

*(Each with detailed implementations)*

---

## Documentation & Compliance

### D1: Complete API Documentation (12 hours)

**File**: `docs/serving/responses_api.md`

_Status: âœ… Drafted a full reference covering endpoints, request model, SSE events,
tool calling, rate limits, storage, and compatibility mode._

### D2: Migration Guide for Existing Users (8 hours)

**File**: `docs/migration/responses_api_v2.md`

_Status: âœ… Added migration playbook describing breaking changes, recommended server
flags, client updates, and rollout strategy._

### D3: OpenAI Compatibility Documentation (8 hours)

**File**: `docs/source/compatibility/openai_responses_api.md`

Document detailing:
- 100% compatibility statement
- Feature comparison matrix
- Known differences (if any)
- Compliance test results
- Version compatibility

### D4: Code Examples and Tutorials (8 hours)

**File**: `examples/responses_api/`

Examples for:
- Basic text generation
- Tool calling workflow
- Reasoning models
- Streaming responses
- Error handling
- Azure deployment
- Production patterns

### D5: Troubleshooting Guide (4 hours)

**File**: `docs/compatibility/openai_responses_api.md`

_Status: âœ… Added compatibility matrix, known differences, and recommended server flags._

**File**: `examples/responses_api/README.md`

_Status: âœ… Streaming/tool-call/background code samples using the OpenAI SDK._

**File**: `docs/troubleshooting/responses_api.md`

_Status: âœ… Covers SSE disconnects, tool output timeouts, request-id mismatches, and payload limit errors._

---

## Testing Strategy

### T1: Comprehensive Compliance Test Suite (24 hours)

**File**: `tests/compliance/test_openai_responses_api.py`

_Status: âœ… Added async HTTP tests that hit a running server (base URL + model configurable via env)._

Summary:

- Validates `/responses`, `/responses/{id}`, and `/responses/{id}/tool_outputs`.
- Streams SSE responses to ensure events arrive and the `[DONE]` marker is emitted.
- Confirms `X-Request-Id` echo behavior.
- Documented run instructions in `Pytests_HOWTO.md` (â€œCompliance Suiteâ€ section).
        """Spec line 24: GET /v1/responses/{id} endpoint"""
        response = await client.get("/v1/responses/test_id")
        assert response.status_code in [200, 404]  # Exists

    @pytest.mark.compliance
    @pytest.mark.spec_line(25)
    async def test_post_cancel_endpoint_exists(self, client: AsyncClient):
        """Spec line 25: POST /v1/responses/{id}/cancel endpoint"""
        response = await client.post("/v1/responses/test_id/cancel")
        assert response.status_code in [200, 404]  # Exists


class TestRequestParameters:
    """Test all request parameters are supported"""

    @pytest.mark.compliance
    @pytest.mark.spec_line(34)
    async def test_model_parameter(self, client: AsyncClient):
        """Spec line 34: model parameter"""
        response = await client.post(
            "/v1/responses",
            json={"model": "gpt-4", "input": "test"}
        )
        assert response.status_code == 200

    @pytest.mark.compliance
    @pytest.mark.spec_line(35)
    async def test_instructions_parameter(self, client: AsyncClient):
        """Spec line 35: instructions parameter"""
        response = await client.post(
            "/v1/responses",
            json={"model": "gpt-4", "input": "test", "instructions": "Be helpful"}
        )
        assert response.status_code == 200

    @pytest.mark.compliance
    @pytest.mark.spec_line(36)
    async def test_input_parameter_string(self, client: AsyncClient):
        """Spec line 36: input parameter (string)"""
        response = await client.post(
            "/v1/responses",
            json={"model": "gpt-4", "input": "Hello world"}
        )
        assert response.status_code == 200

    @pytest.mark.compliance
    @pytest.mark.spec_line(36)
    async def test_input_parameter_array(self, client: AsyncClient):
        """Spec line 36: input parameter (array)"""
        response = await client.post(
            "/v1/responses",
            json={
                "model": "gpt-4",
                "input": [
                    {"type": "message", "role": "user", "content": [{"type": "text", "text": "Hello"}]}
                ]
            }
        )
        assert response.status_code == 200

    @pytest.mark.compliance
    @pytest.mark.spec_line(37)
    async def test_tools_parameter(self, client: AsyncClient):
        """Spec line 37: tools parameter"""
        response = await client.post(
            "/v1/responses",
            json={
                "model": "gpt-4",
                "input": "test",
                "tools": [{
                    "type": "function",
                    "function": {
                        "name": "get_weather",
                        "description": "Get weather",
                        "parameters": {"type": "object", "properties": {}}
                    }
                }]
            }
        )
        assert response.status_code == 200

    @pytest.mark.compliance
    @pytest.mark.spec_line(38)
    async def test_tool_choice_parameter(self, client: AsyncClient):
        """Spec line 38: tool_choice parameter"""
        for choice in ["auto", "none", "required"]:
            response = await client.post(
                "/v1/responses",
                json={"model": "gpt-4", "input": "test", "tool_choice": choice}
            )
            assert response.status_code == 200

    @pytest.mark.compliance
    @pytest.mark.spec_line(39)
    async def test_parallel_tool_calls_parameter(self, client: AsyncClient):
        """Spec line 39: parallel_tool_calls parameter"""
        response = await client.post(
            "/v1/responses",
            json={"model": "gpt-4", "input": "test", "parallel_tool_calls": True}
        )
        assert response.status_code == 200

    @pytest.mark.compliance
    @pytest.mark.spec_line(39)
    async def test_reasoning_parameter(self, client: AsyncClient):
        """Spec line 39: reasoning parameter"""
        response = await client.post(
            "/v1/responses",
            json={
                "model": "gpt-4",
                "input": "test",
                "reasoning": {"effort": "medium"}
            }
        )
        assert response.status_code == 200

    @pytest.mark.compliance
    @pytest.mark.spec_line(40)
    async def test_store_parameter(self, client: AsyncClient):
        """Spec line 40: store parameter"""
        response = await client.post(
            "/v1/responses",
            json={"model": "gpt-4", "input": "test", "store": True}
        )
        assert response.status_code == 200
        data = response.json()
        # Should be able to retrieve later
        response_id = data["id"]

        get_response = await client.get(f"/v1/responses/{response_id}")
        assert get_response.status_code == 200

    @pytest.mark.compliance
    @pytest.mark.spec_line(41)
    async def test_stream_parameter(self, client: AsyncClient):
        """Spec line 41: stream parameter"""
        response = await client.post(
            "/v1/responses",
            json={"model": "gpt-4", "input": "test", "stream": True},
            headers={"Accept": "text/event-stream"}
        )
        assert response.status_code == 200
        assert "text/event-stream" in response.headers.get("content-type", "")

    @pytest.mark.compliance
    @pytest.mark.spec_line(42)
    async def test_include_parameter(self, client: AsyncClient):
        """Spec line 42: include parameter"""
        response = await client.post(
            "/v1/responses",
            json={
                "model": "gpt-4",
                "input": "test",
                "include": ["reasoning.encrypted_content"]
            }
        )
        assert response.status_code == 200

    @pytest.mark.compliance
    @pytest.mark.spec_line(43)
    async def test_prompt_cache_key_parameter(self, client: AsyncClient):
        """Spec line 43: prompt_cache_key parameter"""
        cache_key = "test_cache_key_001"

        # First request
        response1 = await client.post(
            "/v1/responses",
            json={
                "model": "gpt-4",
                "input": "Long prompt for caching",
                "prompt_cache_key": cache_key
            }
        )
        assert response1.status_code == 200
        usage1 = response1.json()["usage"]

        # Second request with same key
        response2 = await client.post(
            "/v1/responses",
            json={
                "model": "gpt-4",
                "input": "Long prompt for caching",
                "prompt_cache_key": cache_key
            }
        )
        assert response2.status_code == 200
        usage2 = response2.json()["usage"]

        # Second request should have cached tokens
        assert usage2["input_tokens_details"]["cached_tokens"] > 0

    @pytest.mark.compliance
    @pytest.mark.spec_line(44)
    async def test_text_parameter(self, client: AsyncClient):
        """Spec line 44: text parameter"""
        response = await client.post(
            "/v1/responses",
            json={
                "model": "gpt-4",
                "input": "test",
                "text": {"verbosity": "high"}
            }
        )
        assert response.status_code == 200


class TestSSEEvents:
    """Test all SSE events are emitted correctly"""

    @pytest.mark.compliance
    @pytest.mark.spec_line(68)
    async def test_response_created_event(self, client: AsyncClient):
        """Spec line 68: response.created event"""
        response = await client.post(
            "/v1/responses",
            json={"model": "gpt-4", "input": "test", "stream": True},
            headers={"Accept": "text/event-stream"}
        )

        async for line in response.aiter_lines():
            if "response.created" in line:
                # Next line has data
                continue
            if line.startswith("data: "):
                data = json.loads(line[6:])
                if data.get("type") == "response.created":
                    assert "response" in data
                    assert "id" in data["response"]
                    assert data["response"]["status"] == "in_progress"
                    return

        pytest.fail("response.created event not found")

    @pytest.mark.compliance
    @pytest.mark.spec_line(75)
    async def test_output_text_delta_event(self, client: AsyncClient):
        """Spec line 75: response.output_text.delta event"""
        response = await client.post(
            "/v1/responses",
            json={"model": "gpt-4", "input": "Say hello", "stream": True},
            headers={"Accept": "text/event-stream"}
        )

        found = False
        async for line in response.aiter_lines():
            if "response.output_text.delta" in line:
                found = True
                continue
            if found and line.startswith("data: "):
                data = json.loads(line[6:])
                assert "delta" in data
                break

        assert found, "response.output_text.delta event not found"

    @pytest.mark.compliance
    @pytest.mark.spec_line(91)
    async def test_tool_call_delta_event(self, client: AsyncClient):
        """Spec line 91: response.tool_call.delta event"""
        response = await client.post(
            "/v1/responses",
            json={
                "model": "gpt-4",
                "input": "Call the weather function",
                "tools": [{
                    "type": "function",
                    "function": {
                        "name": "get_weather",
                        "description": "Get weather",
                        "parameters": {"type": "object", "properties": {}}
                    }
                }],
                "stream": True
            },
            headers={"Accept": "text/event-stream"}
        )

        found = False
        async for line in response.aiter_lines():
            if "response.tool_call.delta" in line:
                found = True
                continue
            if found and line.startswith("data: "):
                data = json.loads(line[6:])
                assert "delta" in data
                assert "content" in data["delta"]
                tool_call = data["delta"]["content"][0]
                assert tool_call["type"] == "tool_call"
                assert "id" in tool_call
                assert "name" in tool_call
                break

        assert found, "response.tool_call.delta event not found"

    @pytest.mark.compliance
    @pytest.mark.spec_line(112)
    async def test_reasoning_delta_event(self, client: AsyncClient):
        """Spec line 112: response.reasoning.delta event"""
        response = await client.post(
            "/v1/responses",
            json={
                "model": "gpt-4",
                "input": "Solve a complex problem",
                "reasoning": {"effort": "high"},
                "stream": True
            },
            headers={"Accept": "text/event-stream"}
        )

        found = False
        async for line in response.aiter_lines():
            if "response.reasoning.delta" in line:
                found = True
                continue
            if found and line.startswith("data: "):
                data = json.loads(line[6:])
                assert "delta" in data
                assert "content" in data["delta"]
                break

        assert found, "response.reasoning.delta event not found"

    @pytest.mark.compliance
    @pytest.mark.spec_line(120)
    async def test_reasoning_summary_delta_event(self, client: AsyncClient):
        """Spec line 120: response.reasoning.summary.delta event"""
        # ... test implementation

    @pytest.mark.compliance
    @pytest.mark.spec_line(155)
    async def test_error_event(self, client: AsyncClient):
        """Spec line 155: response.error event"""
        response = await client.post(
            "/v1/responses",
            json={
                "model": "gpt-4",
                "input": "test",
                "temperature": 10.0,  # Invalid - triggers error
                "stream": True
            },
            headers={"Accept": "text/event-stream"}
        )

        found = False
        async for line in response.aiter_lines():
            if "response.error" in line:
                found = True
                continue
            if found and line.startswith("data: "):
                data = json.loads(line[6:])
                assert "error" in data
                assert "type" in data["error"]
                assert "message" in data["error"]
                break

        assert found, "response.error event not found"

    @pytest.mark.compliance
    @pytest.mark.spec_line(166)
    async def test_completed_event(self, client: AsyncClient):
        """Spec line 166: response.completed event"""
        response = await client.post(
            "/v1/responses",
            json={"model": "gpt-4", "input": "test", "stream": True},
            headers={"Accept": "text/event-stream"}
        )

        found = False
        async for line in response.aiter_lines():
            if "response.completed" in line:
                found = True
                continue
            if found and line.startswith("data: "):
                data = json.loads(line[6:])
                assert data["response"]["status"] == "completed"
                assert "usage" in data
                assert "input_tokens" in data["usage"]
                assert "output_tokens" in data["usage"]
                assert "total_tokens" in data["usage"]
                break

        assert found, "response.completed event not found"

    @pytest.mark.compliance
    @pytest.mark.spec_line(182)
    async def test_additional_context_event(self, client: AsyncClient):
        """Spec line 182: response.additional_context event"""
        response = await client.post(
            "/v1/responses",
            json={
                "model": "gpt-4",
                "input": "test",
                "reasoning": {"effort": "high"},
                "include": ["reasoning.encrypted_content"],
                "stream": True
            },
            headers={"Accept": "text/event-stream"}
        )

        found = False
        async for line in response.aiter_lines():
            if "response.additional_context" in line:
                found = True
                continue
            if found and line.startswith("data: "):
                data = json.loads(line[6:])
                assert "context" in data
                assert "reasoning.encrypted_content" in data["context"]
                break

        assert found, "response.additional_context event not found"

    @pytest.mark.compliance
    @pytest.mark.spec_line(193)
    async def test_rate_limits_updated_event(self, client: AsyncClient):
        """Spec line 193: response.rate_limits.updated event"""
        response = await client.post(
            "/v1/responses",
            json={"model": "gpt-4", "input": "test", "stream": True},
            headers={"Accept": "text/event-stream"}
        )

        found = False
        async for line in response.aiter_lines():
            if "response.rate_limits.updated" in line:
                found = True
                continue
            if found and line.startswith("data: "):
                data = json.loads(line[6:])
                assert "limits" in data
                assert "primary" in data["limits"]
                assert "used_percent" in data["limits"]["primary"]
                break

        assert found, "response.rate_limits.updated event not found"


class TestToolCallingWorkflow:
    """Test complete tool calling workflow"""

    @pytest.mark.compliance
    @pytest.mark.spec_line(211)
    async def test_complete_tool_workflow(self, client: AsyncClient):
        """Spec line 211-220: Complete tool calling workflow"""

        # Step 1: Create response with tool
        response = await client.post(
            "/v1/responses",
            json={
                "model": "gpt-4",
                "input": "What's the weather?",
                "tools": [{
                    "type": "function",
                    "function": {
                        "name": "get_weather",
                        "parameters": {"type": "object", "properties": {"location": {"type": "string"}}}
                    }
                }],
                "stream": True
            },
            headers={"Accept": "text/event-stream"}
        )

        # Step 2: Parse tool call from stream
        tool_call_id = None
        response_id = None

        async for line in response.aiter_lines():
            if line.startswith("data: "):
                data = json.loads(line[6:])

                if data.get("type") == "response.created":
                    response_id = data["response"]["id"]

                if data.get("type") == "response.tool_call.delta":
                    tool_call = data["delta"]["content"][0]
                    tool_call_id = tool_call["id"]
                    break

        assert tool_call_id is not None, "Tool call not found in stream"
        assert response_id is not None, "Response ID not found"

        # Step 3: Submit tool outputs
        tool_response = await client.post(
            f"/v1/responses/{response_id}/tool_outputs",
            json={
                "tool_call_id": tool_call_id,
                "output": [
                    {"type": "output_text", "text": "Sunny, 75Â°F"}
                ]
            }
        )

        assert tool_response.status_code == 200

        # Step 4: Verify stream continues (read remaining events)
        event_after_tool = False
        async for line in response.aiter_lines():
            if line.startswith("data: "):
                data = json.loads(line[6:])
                if data.get("type") == "response.output_text.delta":
                    event_after_tool = True
                    break

        assert event_after_tool, "Stream did not continue after tool outputs"


class TestErrorHandling:
    """Test error handling compliance"""

    @pytest.mark.compliance
    @pytest.mark.spec_line(228)
    async def test_429_retry_after_header(self, client: AsyncClient):
        """Spec line 228: 429 returns Retry-After header"""
        # Trigger rate limit by making many requests
        for _ in range(100):
            response = await client.post(
                "/v1/responses",
                json={"model": "gpt-4", "input": "test"}
            )
            if response.status_code == 429:
                assert "retry-after" in response.headers
                break

    @pytest.mark.compliance
    @pytest.mark.spec_line(233)
    async def test_special_error_types(self, client: AsyncClient):
        """Spec line 233: Special error types"""
        # Test usage_limit_reached
        # Test quota_exceeded
        # Test invalid_api_key
        # ... etc
        pass


class TestAzureCompatibility:
    """Test Azure-specific features"""

    @pytest.mark.compliance
    @pytest.mark.spec_line(257)
    async def test_azure_endpoint_format(self, client: AsyncClient):
        """Spec line 257: Azure endpoint format"""
        response = await client.post(
            "/openai/deployments/gpt-4/responses?api-version=2024-02-15-preview",
            json={"input": "test"},
            headers={"api-key": "test_key"}
        )
        assert response.status_code == 200

    @pytest.mark.compliance
    @pytest.mark.spec_line(259)
    async def test_azure_forces_store_true(self, client: AsyncClient):
        """Spec line 259: Azure forces store=true"""
        response = await client.post(
            "/openai/deployments/gpt-4/responses?api-version=2024-02-15-preview",
            json={"input": "test", "store": False},
            headers={"api-key": "test_key"}
        )
        # Should be stored despite store=false
        assert response.json().get("store") == True


# Run compliance tests
if __name__ == "__main__":
    pytest.main([__file__, "-v", "-m", "compliance"])
```

---

## Implementation Timeline

### Phase 1: Critical Features (Weeks 1-8)

**Week 1-2**: Session Management
- Implement ResponseSession class
- SessionManager with lifecycle management
- Session storage and cleanup
- **Deliverable**: Session infrastructure ready

**Week 3-4**: Tool Outputs Endpoint
- Add `/v1/responses/{id}/tool_outputs` route
- Integrate with session management
- Tool output queuing and processing
- **Deliverable**: Tool outputs endpoint working

**Week 5-6**: Tool Call Delta Events
- Implement `response.tool_call.delta` event
- Update tool parsers for streaming detection
- SSE serialization for tool calls
- **Deliverable**: Streaming tool calls working

**Week 7-8**: Error Events & Testing
- Implement `response.error` SSE event
- Error handling in streams
- Integration testing
- **Deliverable**: Phase 1 complete, 40% compatibility

### Phase 2: High Priority Features (Weeks 9-12)

**Week 9-10**: Prompt Caching
- Implement PromptCache class
- Integration with tokenization
- Cache hit tracking
- **Deliverable**: Prompt caching working

**Week 11-12**: Reasoning Events
- Rename to OpenAI format
- Add summary events
- Additional context events
- Rate limit events
- **Deliverable**: Phase 2 complete, 70% compatibility

### Phase 3: Medium Priority (Weeks 13-15)

**Week 13**: Azure & Error Types
- Azure endpoint format
- OpenAI error types
- Retry-After headers
- **Deliverable**: Azure compatibility

**Week 14**: Include Parameter & Headers
- All include options
- HTTP headers compatibility
- Store parameter semantics
- **Deliverable**: Full parameter support

**Week 15**: Testing & Bug Fixes
- Medium priority testing
- Bug fixes from testing
- **Deliverable**: Phase 3 complete, 85% compatibility

### Phase 4: Polish & Documentation (Weeks 16-17)

**Week 16**: Low Priority & Edge Cases
- All L1-L8 tasks
- All E1-E8 tasks
- **Deliverable**: 95% compatibility

**Week 17**: Documentation & Compliance
- Complete documentation (D1-D5)
- Compliance test suite (T1)
- Final testing
- **Deliverable**: 100% compatibility, production-ready

### Phase 5: Release (Week 18)

- Release candidate testing
- Performance benchmarking
- Migration guide finalization
- Official release
- **Deliverable**: vLLM Responses API v2.0 with 100% OpenAI compatibility

---

## Success Criteria

### Functional Requirements

âœ… **All 4 endpoints implemented**
âœ… **All request parameters supported**
âœ… **All SSE events emitted correctly**
âœ… **Tool calling workflow matches OpenAI**
âœ… **Error handling matches OpenAI**
âœ… **Azure compatibility complete**

### Quality Requirements

âœ… **100% test coverage for compliance**
âœ… **All OpenAI compliance tests pass**
âœ… **Performance: <5% overhead vs current**
âœ… **Memory: <100MB for 1000 sessions**
âœ… **Documentation: Complete and accurate**

### User Requirements

âœ… **Zero breaking changes with compat mode**
âœ… **Migration guide: Clear and complete**
âœ… **Examples: Cover all use cases**
âœ… **Troubleshooting: Common issues documented**

---

## Conclusion

This roadmap provides a **complete, detailed plan** for achieving **100% OpenAI Responses API compatibility** in vLLM.

**Total Effort**: 480 hours (60 working days, ~3 months with 1 developer)

**Deliverables**:
- 35 implementation tasks completed
- 100% OpenAI compatibility
- Complete documentation
- Comprehensive test suite
- Migration guide
- Production-ready release

Follow this plan systematically to transform vLLM into a **perfect drop-in replacement** for OpenAI's Responses API.
