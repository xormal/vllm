# Руководство по лимитам (RateLimiting_HOWTO)

Эти шаги позволяют включать/выключать ограничение запросов и токенов в
Responses API.

## 1. Конфигурация (JSON)

В репозитории лежит пример `rate_limits_config.json`:

```json
{
  "enabled": true,
  "enforce": true,
  "request_limits": {
    "enabled": true,
    "per_minute": 60,
    "per_hour": 1000
  },
  "token_limits": {
    "enabled": true,
    "per_minute": 100000
  }
}
```

Поля:

| Ключ | Назначение |
|------|------------|
| `enabled` | Включает/выключает чтение конфига (если `false`, работает только трекинг событий). |
| `enforce` | Включает строгие ограничения (429 + Retry-After). При `false` сервер только считает usage. |
| `request_limits.enabled` | Управляет лимитом по числу запросов; можно отключить, оставив только квоту по токенам. |
| `request_limits.per_minute`, `per_hour` | Бюджеты для окон 1 мин/1 час. |
| `token_limits.enabled` | Отвечает за лимит по токенам/минуту. |
| `token_limits.per_minute` | Максимум выходных токенов в минуту. |

Измените значения под свои цели и сохраните файл (можно создать несколько профилей).

## 2. Запуск сервера

Передайте путь к конфигу в фронтенд:

```bash
python -m vllm.entrypoints.openai.api_server \
  --model <MODEL> \
  --rate-limits-config /path/to/rate_limits_config.json
```

Если нужно только собирать метрики (без 429), выставьте `"enforce": false`. Для полного отключения достаточно поставить `"enabled": false` или не передавать `--rate-limits-config`.

Дополнительно можно включить SSE-ивенты `response.rate_limits.updated` (для мониторинга) флагом `--enable-rate-limit-events`.

## 3. Проверка

1. Отправьте несколько запросов с `user` идентификатором, чтобы исчерпать дневной лимит.
2. После превышения сервер вернёт 429 и заголовок `Retry-After` (значение берётся из ближайшего восстанавливающегося окна).
3. Снимите ограничение, изменив JSON (напр. `enabled: false`) и перезапустив сервер либо укажите другой конфиг.
